{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2783,
     "status": "ok",
     "timestamp": 1620954452672,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "DWeqxtvvXUxb",
    "outputId": "0734e95d-a92f-4014-fa26-4d643e5677de"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1620954455830,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "Bwz10m0AmDwJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Text cleaning\n",
    "\n",
    "def cleanStr(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1620954457944,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "7F3-yWdZmFGX"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_folder = [\"rt-polarity.pos\", \"rt-polarity.neg\"]\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "path = 'MovieReviews/rt-polaritydata/'\n",
    "\n",
    "pos_file = data_folder[0]\n",
    "neg_file = data_folder[1]\n",
    "\n",
    "with open(path + pos_file, \"rb\") as f:\n",
    "    for line in f:       \n",
    "        temp = line.strip()\n",
    "        temp = temp.decode(\"latin-1\") \n",
    "        temp = cleanStr(temp)\n",
    "\n",
    "        sentences.append(temp)\n",
    "        labels.append(1)\n",
    "\n",
    "with open(path + neg_file, \"rb\") as f:\n",
    "    for line in f:\n",
    "        temp = line.strip()\n",
    "        temp = temp.decode(\"latin-1\") \n",
    "        temp = cleanStr(temp)\n",
    "\n",
    "        sentences.append(temp)\n",
    "        labels.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1620954459931,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "bgQKGqPwmFJP",
    "outputId": "1afbb2a5-77d4-452e-fce1-87abd15efffa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>serry wants to blend politics and drama , an a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8649</th>\n",
       "      <td>an amateurish , quasi improvised acting exerci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>playing a role of almost bergmanesque intensit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>no big whoop , nothing new to see , zero thril...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>please , someone , stop eric schaeffer before ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  sentiment\n",
       "7168  serry wants to blend politics and drama , an a...          0\n",
       "8649  an amateurish , quasi improvised acting exerci...          0\n",
       "3381  playing a role of almost bergmanesque intensit...          1\n",
       "8718  no big whoop , nothing new to see , zero thril...          0\n",
       "5367  please , someone , stop eric schaeffer before ...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(list(zip(sentences, labels)), columns = ['sentence', 'sentiment'])\n",
    "df = df.sample(frac = 1, random_state = 0) # shuffle the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1620954462361,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "AHBZGzyumFMb",
    "outputId": "8dbc92a3-23f7-4bef-ea2f-cd3e4c6ad855"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5331\n",
       "1    5331\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 85062,
     "status": "ok",
     "timestamp": 1620954549074,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "BGpSYHXtcRCk",
    "outputId": "8c306355-8b10-42b0-b3e1-64c8d01075e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>serry wants to blend politics and drama , an a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8649</th>\n",
       "      <td>an amateurish , quasi improvised acting exerci...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>playing a role of almost bergmanesque intensit...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>no big whoop , nothing new to see , zero thril...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>please , someone , stop eric schaeffer before ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  sentiment  \\\n",
       "7168  serry wants to blend politics and drama , an a...          0   \n",
       "8649  an amateurish , quasi improvised acting exerci...          0   \n",
       "3381  playing a role of almost bergmanesque intensit...          1   \n",
       "8718  no big whoop , nothing new to see , zero thril...          0   \n",
       "5367  please , someone , stop eric schaeffer before ...          0   \n",
       "\n",
       "                                                  vader  \n",
       "7168  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "8649  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3381  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "8718  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5367  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# max token count in all sentences = 56\n",
    "\n",
    "def vaderSentiment(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "\n",
    "    if vs['compound'] <= -0.05:\n",
    "        return [0 for i in range(56)]\n",
    "    elif vs['compound'] >= 0.05:\n",
    "        return [2 for i in range(56)]\n",
    "    return [1 for i in range(56)]\n",
    "\n",
    "df['vader'] = df['sentence'].apply(lambda x: vaderSentiment(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1620954558631,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "sKJAFehKnOMW",
    "outputId": "8b57192e-566a-414b-c5a4-bee8718852e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>serry wants to blend politics and drama , an a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8649</th>\n",
       "      <td>an amateurish , quasi improvised acting exerci...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>playing a role of almost bergmanesque intensit...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>no big whoop , nothing new to see , zero thril...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>please , someone , stop eric schaeffer before ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  sentiment  \\\n",
       "7168  serry wants to blend politics and drama , an a...          0   \n",
       "8649  an amateurish , quasi improvised acting exerci...          0   \n",
       "3381  playing a role of almost bergmanesque intensit...          1   \n",
       "8718  no big whoop , nothing new to see , zero thril...          0   \n",
       "5367  please , someone , stop eric schaeffer before ...          0   \n",
       "\n",
       "                                                  vader  match  \n",
       "7168  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      0  \n",
       "8649  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "3381  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      1  \n",
       "8718  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "5367  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VADER\n",
    "# 0: negative\n",
    "# 1: neutral\n",
    "# 2: positive\n",
    "\n",
    "def vaderAccuracy(s, v):\n",
    "    if s == 1 and v[0] == 2:\n",
    "        return 1\n",
    "    elif s == 0 and v[0] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['match'] = df.apply(lambda x: vaderAccuracy(x['sentiment'], x['vader']), axis = 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1620953804605,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "wXyWOj8TM2ma",
    "outputId": "9449a3cb-3055-42f8-cfb3-fe48e3f8f123"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5485837553929844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum(df['match'])/len(df['match'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1620954564286,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "O9_uNrOuLoyL",
    "outputId": "a237d396-a1a7-4804-f23e-07c9c032be22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5331\n",
       "1    5331\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1620954567177,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "yWzJNf3zXaiX",
    "outputId": "448f5f4d-8364-4dae-8140-60b5dec16225"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>[serry, wants, to, blend, politics, and, drama...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8649</th>\n",
       "      <td>[an, amateurish, ,, quasi, improvised, acting,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>[playing, a, role, of, almost, bergmanesque, i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>[no, big, whoop, ,, nothing, new, to, see, ,, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>[please, ,, someone, ,, stop, eric, schaeffer,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  sentiment  \\\n",
       "7168  [serry, wants, to, blend, politics, and, drama...          0   \n",
       "8649  [an, amateurish, ,, quasi, improvised, acting,...          0   \n",
       "3381  [playing, a, role, of, almost, bergmanesque, i...          1   \n",
       "8718  [no, big, whoop, ,, nothing, new, to, see, ,, ...          0   \n",
       "5367  [please, ,, someone, ,, stop, eric, schaeffer,...          0   \n",
       "\n",
       "                                                  vader  match  \n",
       "7168  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      0  \n",
       "8649  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "3381  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...      1  \n",
       "8718  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "5367  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Tokenize the sentences\n",
    "df['sentence'] = df['sentence'].apply(lambda x: word_tokenize(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1620954609619,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "aJAiJG_BoCbp",
    "outputId": "a4073af5-2cbb-45f0-b983-ad3020149eef"
   },
   "outputs": [],
   "source": [
    "\n",
    "sentences = df['sentence'].tolist()\n",
    "#sentences[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1620954612109,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "5qdaXAk1oCxY",
    "outputId": "5d122247-6843-45b8-c805-cf4535f62be2"
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = df['sentiment'].tolist()\n",
    "#labels[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1620954616085,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "3KjYgKMdeMiT",
    "outputId": "9b194ae8-66f2-451f-be74-c6141428cfc7"
   },
   "outputs": [],
   "source": [
    "\n",
    "vaders = df['vader'].tolist()\n",
    "#vaders[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1620954618718,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "HWw3QhtvO4mv",
    "outputId": "49ad15cc-f5cc-4655-dcc7-e19095ced042"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate the maximum number of tokens in a sentence\n",
    "max_len = max([len(s) for s in sentences])\n",
    "max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1620954622637,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "KaFfx9fXYD4A",
    "outputId": "b893e002-cc4b-402a-a516-f53c0c42189b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# unique tokens in the dataset\n",
    "dataset_vocabulary = set([item for sublist in sentences for item in sublist])\n",
    "#dataset_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1620954624986,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "ESDA3gKHaNx5",
    "outputId": "e80b2352-a482-4bb8-cab5-e440c2d89e50"
   },
   "outputs": [],
   "source": [
    "\n",
    "# assign id to each unique token\n",
    "word_index = dict(zip(dataset_vocabulary, range(2, 2 + len(dataset_vocabulary))))\n",
    "#word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1620954629208,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "8q5rIXXm3eFo",
    "outputId": "bce7ae0a-6447-4e48-a1dd-b9abfc05bec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17121  8339  3990  2509  8646 10049 14303 15219 14365  1865   801 11817\n",
      "   3071 17537  3325 13645 18068  8058  8563 10816  1755  3990 16207  7781\n",
      "   7873 14841 10131   914 15701  4480     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [14365 15093 15219   713  4561  1876  8764  6750  6885 14283  8726 12232\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [14250   914 17333  5992  8276  6100  5349   773 14841 16863 12268 10049\n",
      "   7896     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 5415  2768 16768 15219 17258 10355  3990  8303 15219  5803  4054 15219\n",
      "  17537 17264  4179 10049   914  8295  1153  7394 15842   914  3325  9556\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [10517 15219 12803 15219 10564  4262  3501  2093 10816  4038 10592  9556\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "(10662, 56)\n",
      "[0 0 1 0 0]\n",
      "(10662,)\n",
      "[[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "(10662, 56)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences_ready = []\n",
    "vaders_ready = []\n",
    "for sentence, vad in zip(sentences, vaders):\n",
    "    temp = [0 for i in range(max_len)]\n",
    "\n",
    "    for i in range(min(max_len, len(sentence))):\n",
    "        temp[i] = word_index[sentence[i]]\n",
    "\n",
    "    sentences_ready.append(np.array(temp))\n",
    "    vaders_ready.append(np.array(vad))\n",
    "\n",
    "sentences_ready = np.array(sentences_ready)\n",
    "labels_ready = np.array(labels)\n",
    "vaders_ready = np.array(vaders_ready)\n",
    "\n",
    "print(sentences_ready[:5])\n",
    "print(sentences_ready.shape)\n",
    "\n",
    "print(labels_ready[:5])\n",
    "print(labels_ready.shape)\n",
    "\n",
    "print(vaders_ready[:5])\n",
    "print(vaders_ready.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pHiTeyJxQywm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Classification using Logistic Regression\n",
    "def logisticRegression(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    # Create a Logistic Regression classifier\n",
    "    clf = LogisticRegression(random_state = 0, solver = 'liblinear', multi_class = 'ovr', max_iter = 1000)\n",
    "\n",
    "    # Train the model using the training set\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Predict the classes for the test set\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Calculate accuracy of the model\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1620954755458,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "yVJklLU6WWHy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Classification using Support Vector Machines\n",
    "def supportVectorMachines(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    # Create a Support Vector Machines classifier\n",
    "    clf = svm.LinearSVC(random_state = 0, C = 0.01)\n",
    "\n",
    "    # Train the model using the training set\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Predict the classes for the test set\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Calculate accuracy of the model\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1620954632979,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "DeBLevNLYD63"
   },
   "outputs": [],
   "source": [
    "\n",
    "dimension = 300 # 50 100 200 300\n",
    "\n",
    "glove_embeddings = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27006,
     "status": "ok",
     "timestamp": 1620953866189,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "zzizbcesiwOZ",
    "outputId": "43dd7745-dd78-4efb-a7be-fcb45efa5fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load GloVe embeddings into a dictionary\n",
    "\n",
    "f = open('GloVe/glove.6B.' + str(dimension) + 'd.txt', 'r', encoding = 'utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    glove_embeddings[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors in GloVe.' % len(glove_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1620954738337,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "_rM29_3LYD9p",
    "outputId": "bb63d23d-295b-464e-e414-be731840e6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17640 words (944 misses)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the plain word embeddings\n",
    "num_tokens = len(dataset_vocabulary) + 2\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, dimension))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        \"\"\"\n",
    "        For the unknown words create random word vectors.    \n",
    "        0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones.\n",
    "        \"\"\"\n",
    "        embedding_matrix[i] = np.random.uniform(-0.25, 0.25, dimension)\n",
    "        misses += 1\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--o-iC5EWWVZ"
   },
   "source": [
    "### GloVe | Plain | LR & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494610,
     "status": "ok",
     "timestamp": 1618625878960,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "o9JX8zn7XcDJ",
    "outputId": "cfdad720-2136-4110-bf4d-c9fda087de41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Logistic Regression accuracy: 0.6682\n",
      "10-fold CV Support Vector Machines accuracy: 0.6808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(sentences_ready)\n",
    "\n",
    "model_acc_lr = 0\n",
    "model_acc_svm = 0\n",
    "\n",
    "for train_index, test_index in kf.split(sentences_ready):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    plain_x_train, plain_x_test = sentences_ready[train_index], sentences_ready[test_index]\n",
    "    plain_y_train, plain_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    # plain GloVe\n",
    "    logreg_plain_x_train = []\n",
    "\n",
    "    for sent in plain_x_train:\n",
    "        temp = []\n",
    "        for token in sent:\n",
    "            temp.extend(embedding_matrix[token])\n",
    "        \n",
    "        logreg_plain_x_train.append(temp)\n",
    "\n",
    "\n",
    "    logreg_plain_x_test = []\n",
    "\n",
    "    for sent in plain_x_test:\n",
    "        temp = []\n",
    "        for token in sent:\n",
    "            temp.extend(embedding_matrix[token])\n",
    "\n",
    "        logreg_plain_x_test.append(temp)\n",
    "\n",
    "    logreg_plain_y_train = plain_y_train.copy()\n",
    "    logreg_plain_y_test = plain_y_test.copy()\n",
    "   \n",
    "    accuracy_lr = logisticRegression(logreg_plain_x_train, logreg_plain_y_train, logreg_plain_x_test, logreg_plain_y_test)\n",
    "    accuracy_svm = supportVectorMachines(logreg_plain_x_train, logreg_plain_y_train, logreg_plain_x_test, logreg_plain_y_test)\n",
    "\n",
    "    model_acc_lr += accuracy_lr\n",
    "    model_acc_svm += accuracy_svm\n",
    "\n",
    "    #print('Test accuracy LR: %.4f' % accuracy_lr)\n",
    "    #print('Test accuracy SVM: %.4f' % accuracy_svm)\n",
    "\n",
    "print('10-fold CV Logistic Regression accuracy: %.4f' % (model_acc_lr/10))\n",
    "print('10-fold CV Support Vector Machines accuracy: %.4f' % (model_acc_svm/10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJKYv36qqDM6"
   },
   "source": [
    "### GloVe | Plain | CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tDcNLCveqDPu"
   },
   "outputs": [],
   "source": [
    "\n",
    "plain_embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    dimension,\n",
    "    embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n",
    "    input_length = max_len,\n",
    "    trainable = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1338336,
     "status": "ok",
     "timestamp": 1618624197208,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "O7AV4MBLptiy",
    "outputId": "dda7a58b-badc-4007-896e-7f63d08a9242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Convolutional Neural Network accuracy: 0.7738\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(sentences_ready)\n",
    "\n",
    "model_acc = 0\n",
    "\n",
    "for train_index, test_index in kf.split(sentences_ready):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    plain_x_train, plain_x_test = sentences_ready[train_index], sentences_ready[test_index]\n",
    "    plain_y_train, plain_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    plain_x_train, plain_x_val, plain_y_train, plain_y_val = train_test_split(plain_x_train, plain_y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    \n",
    "    ##### Convolutional Neural Network - single input #####\n",
    "    \n",
    "    sequence_input = Input(shape = (max_len,), dtype = 'int64')\n",
    "    embedded_sequences = plain_embedding_layer(sequence_input)\n",
    "    print(embedded_sequences.shape)\n",
    "\n",
    "    # add first conv filter\n",
    "    x = Conv1D(100, 5, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    x = MaxPooling1D(max_len - 5 + 1)(x)\n",
    "\n",
    "    # add second conv filter\n",
    "    y = Conv1D(100, 4, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    y = MaxPooling1D(max_len - 4 + 1)(y)\n",
    "\n",
    "    # add third conv filter\n",
    "    z = Conv1D(100, 3, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    z = MaxPooling1D(max_len - 3 + 1)(z)\n",
    "\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "    # flatted the pooled features\n",
    "    alpha = Flatten()(alpha)\n",
    "\n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "\n",
    "    # predictions\n",
    "    preds = Dense(1, activation = 'sigmoid')(alpha)\n",
    "\n",
    "    # build model\n",
    "    model = Model(inputs = sequence_input, outputs = preds)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.0001,\n",
    "        decay_steps = 100000,\n",
    "        decay_rate = 0.95,\n",
    "        staircase = True)\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                      optimizer = optimizer,\n",
    "                      metrics = ['acc'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_test_acc = 0\n",
    "    test_acc = None\n",
    "\n",
    "    for m in range(25):\n",
    "        history = model.fit(plain_x_train,\n",
    "              plain_y_train,\n",
    "              batch_size = 50, \n",
    "              epochs = 1, \n",
    "              validation_data = (plain_x_val, plain_y_val))\n",
    "        \n",
    "        if best_val_loss is None or history.history['val_loss'][0] < best_val_loss:\n",
    "            best_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "            _, test_acc = model.evaluate(plain_x_test, plain_y_test, batch_size = 50)\n",
    "\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "\n",
    "    model_acc += best_test_acc\n",
    "\n",
    "    #print('Test accuracy: %.4f' % best_test_acc)\n",
    "\n",
    "print('10-fold CV Convolutional Neural Network accuracy: %.4f' % (model_acc/10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10275,
     "status": "ok",
     "timestamp": 1620955125071,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "LJPHIzgfKAkO",
    "outputId": "47757f95-f983-4130-a983-b1c715ced6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 218520 words (0 misses)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add context\n",
    "\n",
    "context_len = 5\n",
    "\n",
    "context_word_index = {}\n",
    "context_embedding_dict = {}\n",
    "\n",
    "sentences_ready = []\n",
    "\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "word_index_count = 2\n",
    "\n",
    "for sentence in sentences:\n",
    "\n",
    "    indexed_sentence = [0 for i in range(max_len)]\n",
    "\n",
    "    for index, token in enumerate(sentence):\n",
    "\n",
    "        found = False\n",
    "        vec = [0 for i in range(dimension)]\n",
    "        count = 0\n",
    "\n",
    "        item = []\n",
    "\n",
    "        for i in range(index-context_len, index+context_len+1):\n",
    "            if i >= 0 and i < len(sentence):\n",
    "                item.append(sentence[i])\n",
    "                if sentence[i] in glove_embeddings:\n",
    "                    vec += glove_embeddings[sentence[i]]\n",
    "                    count += 1\n",
    "                    found = True\n",
    "        \n",
    "        if not found:\n",
    "            \"\"\"\n",
    "            For the unknown words create random word vectors.    \n",
    "            0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones.\n",
    "            \"\"\"\n",
    "            vec = np.random.uniform(-0.25, 0.25, dimension)\n",
    "            misses += 1\n",
    "        else:\n",
    "            hits += 1\n",
    "\n",
    "        item_tuple = tuple(item)\n",
    "        #print(item_tuple)\n",
    "\n",
    "        if item_tuple not in context_word_index:\n",
    "            context_word_index[item_tuple] = word_index_count\n",
    "            \n",
    "            if count > 1:\n",
    "                vec /= count\n",
    "            \n",
    "            context_embedding_dict[word_index_count] = vec\n",
    "\n",
    "            word_index_count += 1\n",
    "        \n",
    "        indexed_sentence[index] = context_word_index[item_tuple]\n",
    "    \n",
    "    sentences_ready.append(np.array(indexed_sentence))\n",
    "\n",
    "\n",
    "sentences_ready = np.array(sentences_ready)\n",
    "labels_ready = np.array(labels)\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1620955184308,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "Ixh8U0QrKAy0"
   },
   "outputs": [],
   "source": [
    "\n",
    "context_embedding_matrix = np.zeros((len(context_word_index) + 2, dimension))\n",
    "\n",
    "for i, val in context_embedding_dict.items():\n",
    "    context_embedding_matrix[i] = val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe | Context-refined | LR & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689121,
     "status": "ok",
     "timestamp": 1618629948829,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "Qpftcq3NWWvW",
    "outputId": "b96e77bf-a955-49df-b99f-70fa47a88f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Logistic Regression accuracy: 0.7160\n",
      "10-fold CV Support Vector Machines accuracy: 0.7364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(sentences_ready)\n",
    "\n",
    "model_acc_lr = 0\n",
    "model_acc_svm = 0\n",
    "\n",
    "for train_index, test_index in kf.split(sentences_ready):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    context_x_train, context_x_test = sentences_ready[train_index], sentences_ready[test_index]\n",
    "    context_y_train, context_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    # context GloVe\n",
    "    logreg_context_x_train = []\n",
    "\n",
    "    for sent in context_x_train:\n",
    "        temp = []\n",
    "        for token in sent:\n",
    "            temp.extend(context_embedding_matrix[token])\n",
    "        \n",
    "        logreg_context_x_train.append(temp)\n",
    "\n",
    "\n",
    "    logreg_context_x_test = []\n",
    "\n",
    "    for sent in context_x_test:\n",
    "        temp = []\n",
    "        for token in sent:\n",
    "            temp.extend(context_embedding_matrix[token])\n",
    "\n",
    "        logreg_context_x_test.append(temp)\n",
    "\n",
    "    logreg_context_y_train = context_y_train.copy()\n",
    "    logreg_context_y_test = context_y_test.copy()\n",
    "\n",
    "\n",
    "    accuracy_lr = logisticRegression(logreg_context_x_train, logreg_context_y_train, logreg_context_x_test, logreg_context_y_test)\n",
    "    accuracy_svm = supportVectorMachines(logreg_context_x_train, logreg_context_y_train, logreg_context_x_test, logreg_context_y_test)\n",
    "\n",
    "    model_acc_lr += accuracy_lr\n",
    "    model_acc_svm += accuracy_svm\n",
    "\n",
    "    #print('Test accuracy LR: %.4f' % accuracy_lr)\n",
    "    #print('Test accuracy SVM: %.4f' % accuracy_svm)\n",
    "\n",
    "print('10-fold CV Logistic Regression accuracy: %.4f' % (model_acc_lr/10))\n",
    "print('10-fold CV Support Vector Machines accuracy: %.4f' % (model_acc_svm/10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe | Context-refined | CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HxgYHoulQyZ-"
   },
   "outputs": [],
   "source": [
    "\n",
    "context_embedding_layer = Embedding(\n",
    "    len(context_word_index) + 2,\n",
    "    dimension,\n",
    "    embeddings_initializer = keras.initializers.Constant(context_embedding_matrix),\n",
    "    input_length = max_len,\n",
    "    trainable = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1525608,
     "status": "ok",
     "timestamp": 1618627874303,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "naqqYdCxQyc0",
    "outputId": "140356b1-c956-4287-bf49-e1d551eab9d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Convolutional Neural Network accuracy: 0.7764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(sentences_ready)\n",
    "\n",
    "model_acc = 0\n",
    "\n",
    "for train_index, test_index in kf.split(sentences_ready):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    context_x_train, context_x_test = sentences_ready[train_index], sentences_ready[test_index]\n",
    "    context_y_train, context_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    context_x_train, context_x_val, context_y_train, context_y_val = train_test_split(context_x_train, context_y_train, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "\n",
    "    ##### Convolutional Neural Network - single input #####\n",
    "    \n",
    "    sequence_input = Input(shape = (max_len,), dtype = 'int64')\n",
    "    embedded_sequences = context_embedding_layer(sequence_input)\n",
    "    print(embedded_sequences.shape)\n",
    "\n",
    "    # add first conv filter\n",
    "    x = Conv1D(100, 5, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    x = MaxPooling1D(max_len - 5 + 1)(x)\n",
    "\n",
    "    # add second conv filter\n",
    "    y = Conv1D(100, 4, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    y = MaxPooling1D(max_len - 4 + 1)(y)\n",
    "\n",
    "    # add third conv filter\n",
    "    z = Conv1D(100, 3, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    z = MaxPooling1D(max_len - 3 + 1)(z)\n",
    "\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "    # flatted the pooled features\n",
    "    alpha = Flatten()(alpha)\n",
    "\n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "\n",
    "    # predictions\n",
    "    preds = Dense(1, activation = 'sigmoid')(alpha)\n",
    "\n",
    "    # build model\n",
    "    model = Model(inputs = sequence_input, outputs = preds)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.0001,\n",
    "        decay_steps = 100000,\n",
    "        decay_rate = 0.95,\n",
    "        staircase = True)\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                      optimizer = optimizer,\n",
    "                      metrics = ['acc'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_test_acc = 0\n",
    "    test_acc = None\n",
    "\n",
    "    for m in range(25):\n",
    "        history = model.fit(context_x_train,\n",
    "              context_y_train,\n",
    "              batch_size = 50, \n",
    "              epochs = 1, \n",
    "              validation_data = (context_x_val, context_y_val))\n",
    "        \n",
    "        if best_val_loss is None or history.history['val_loss'][0] < best_val_loss:\n",
    "            best_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "            _, test_acc = model.evaluate(context_x_test, context_y_test, batch_size = 50)\n",
    "\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "\n",
    "    model_acc += best_test_acc\n",
    "\n",
    "    #print('Test accuracy: %.4f' % best_test_acc)\n",
    "\n",
    "print('10-fold CV Convolutional Neural Network accuracy: %.4f' % (model_acc/10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eltmeLXWgK3k"
   },
   "source": [
    "## Sentiment & context-refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe | Sentiment & context-refined | LR & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639584,
     "status": "ok",
     "timestamp": 1618662549369,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "yQESybzQAWT7",
    "outputId": "7b5a1115-44da-4e2d-8fca-56531f0daed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Logistic Regression accuracy: 0.7205\n",
      "10-fold CV Support Vector Machines accuracy: 0.7418\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(sentences_ready)\n",
    "\n",
    "model_acc_lr = 0\n",
    "model_acc_svm = 0\n",
    "\n",
    "for train_index, test_index in kf.split(sentences_ready):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    context_x_train, context_x_test = sentences_ready[train_index], sentences_ready[test_index]\n",
    "    context_y_train, context_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    vader_train, vader_test = vaders_ready[train_index], vaders_ready[test_index]\n",
    "\n",
    "    # vader + context GloVe\n",
    "    logreg_vader_x_train = []\n",
    "\n",
    "    for sentiment_sent, context_sent in zip(vader_train, context_x_train):\n",
    "        temp = []\n",
    "        for token in sentiment_sent:\n",
    "            temp.extend(sentiment_sent)\n",
    "        \n",
    "        for token in context_sent:\n",
    "            temp.extend(context_embedding_matrix[token])\n",
    "        \n",
    "        logreg_vader_x_train.append(temp)\n",
    "\n",
    "\n",
    "    logreg_vader_x_test = []\n",
    "\n",
    "    for sentiment_sent, context_sent in zip(vader_test, context_x_test):\n",
    "        temp = []\n",
    "        for token in sentiment_sent:\n",
    "            temp.extend(sentiment_sent)\n",
    "        \n",
    "        for token in context_sent:\n",
    "            temp.extend(context_embedding_matrix[token])\n",
    "        \n",
    "        logreg_vader_x_test.append(temp)\n",
    "\n",
    "\n",
    "    logreg_vader_y_train = context_y_train.copy()\n",
    "    logreg_vader_y_test = context_y_test.copy()\n",
    "\n",
    "\n",
    "    accuracy_lr = logisticRegression(logreg_vader_x_train, logreg_vader_y_train, logreg_vader_x_test, logreg_vader_y_test)\n",
    "    accuracy_svm = supportVectorMachines(logreg_vader_x_train, logreg_vader_y_train, logreg_vader_x_test, logreg_vader_y_test)\n",
    "\n",
    "    model_acc_lr += accuracy_lr\n",
    "    model_acc_svm += accuracy_svm\n",
    "\n",
    "    #print('Test accuracy LR: %.4f' % accuracy_lr)\n",
    "    #print('Test accuracy SVM: %.4f' % accuracy_svm)\n",
    "\n",
    "print('10-fold CV Logistic Regression accuracy: %.4f' % (model_acc_lr/10))\n",
    "print('10-fold CV Support Vector Machines accuracy: %.4f' % (model_acc_svm/10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1618660339027,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "VRLXTZefjwPF",
    "outputId": "fe783b88-cc73-4d3d-b8de-9b61fd000946"
   },
   "outputs": [],
   "source": [
    "\n",
    "vader_embedding_layer = Embedding(\n",
    "    3,\n",
    "    1,\n",
    "    input_length = max_len,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe | Sentiment & context-refined | CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1395436,
     "status": "ok",
     "timestamp": 1618661761288,
     "user": {
      "displayName": "Ayça Deniz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWQovfKNepyukUMdVqyJrIEk57WKKqVDzeco2E2w=s64",
      "userId": "14474235690795676203"
     },
     "user_tz": -60
    },
    "id": "KdhD3RvLQyfk",
    "outputId": "db31dd4e-5c07-4c77-ba91-a7ec42ff7036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Convolutional Neural Network accuracy: 0.7796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "kf.get_n_splits(sentences_ready)\n",
    "\n",
    "model_acc = 0\n",
    "\n",
    "for train_index, test_index in kf.split(sentences_ready):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    context_x_train, context_x_test = sentences_ready[train_index], sentences_ready[test_index]\n",
    "    context_y_train, context_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    context_x_train, context_x_val, context_y_train, context_y_val = train_test_split(context_x_train, context_y_train, test_size=0.2, random_state = 42)\n",
    "    \n",
    "\n",
    "    vader_x_train, vader_x_test = vaders_ready[train_index], vaders_ready[test_index]\n",
    "    vader_y_train, vader_y_test = labels_ready[train_index], labels_ready[test_index]\n",
    "\n",
    "    vader_x_train, vader_x_val, vader_y_train, vader_y_val = train_test_split(vader_x_train, vader_y_train, test_size=0.2, random_state = 42)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Convolutional Neural Network - multiple input #####\n",
    "    \n",
    "    # sentiment word embeddings\n",
    "    vader_input = Input(shape = (max_len,), dtype = 'float32')\n",
    "    vader_embedded_sequences = vader_embedding_layer(vader_input)\n",
    "\n",
    "    # context word embeddings\n",
    "    context_input = Input(shape = (max_len,), dtype = 'int64')\n",
    "    context_embedded_sequences = context_embedding_layer(context_input)\n",
    "\n",
    "    # concate the word embeddings\n",
    "    embedded_sequences = concatenate([vader_embedded_sequences, context_embedded_sequences])\n",
    "\n",
    "    # add first conv filter\n",
    "    x = Conv1D(100, 5, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    x = MaxPooling1D(max_len - 5 + 1)(x)\n",
    "\n",
    "    # add second conv filter\n",
    "    y = Conv1D(100, 4, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    y = MaxPooling1D(max_len - 4 + 1)(y)\n",
    "\n",
    "    # add third conv filter\n",
    "    z = Conv1D(100, 3, activation = 'relu', kernel_constraint = max_norm(3))(embedded_sequences)\n",
    "    z = MaxPooling1D(max_len - 3 + 1)(z)\n",
    "\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "    # flatted the pooled features\n",
    "    alpha = Flatten()(alpha)\n",
    "\n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "\n",
    "    # predictions\n",
    "    preds = Dense(1, activation = 'sigmoid')(alpha)\n",
    "\n",
    "    # build model\n",
    "    model = Model(inputs = [vader_input, context_input], outputs = preds)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.0001,\n",
    "        decay_steps = 100000,\n",
    "        decay_rate = 0.95,\n",
    "        staircase = True)\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate = lr_schedule)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                      optimizer = optimizer,\n",
    "                      metrics = ['acc'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    best_val_loss = None\n",
    "    best_test_acc = 0\n",
    "    test_acc = None\n",
    "\n",
    "    for m in range(25):\n",
    "        history = model.fit([vader_x_train, context_x_train],\n",
    "              context_y_train,\n",
    "              batch_size = 50, \n",
    "              epochs = 1, \n",
    "              validation_data = ([vader_x_val, context_x_val], context_y_val))\n",
    "        \n",
    "        if best_val_loss is None or history.history['val_loss'][0] < best_val_loss:\n",
    "            best_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "            _, test_acc = model.evaluate([vader_x_test, context_x_test], context_y_test, batch_size = 50)\n",
    "\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "\n",
    "    model_acc += best_test_acc\n",
    "\n",
    "    #print('Test accuracy: %.4f' % best_test_acc)\n",
    "\n",
    "print('10-fold CV Convolutional Neural Network accuracy: %.4f' % (model_acc/10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7bOwP1xwK6o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONriDRij/39bNL41Vqbk2z",
   "mount_file_id": "1_-c9AvMkexhMSBFysXPiQDoLYnjHCUXS",
   "name": "FINAL_FINAL_MR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
